{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:09:50.403713Z","iopub.status.busy":"2023-08-17T12:09:50.403360Z","iopub.status.idle":"2023-08-17T12:10:00.586572Z","shell.execute_reply":"2023-08-17T12:10:00.585422Z","shell.execute_reply.started":"2023-08-17T12:09:50.403687Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]}],"source":["import os\n","import cv2\n","import csv\n","import math\n","import scipy\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","import mediapipe as mp\n","import tensorflow as tf\n","import plotly.express as px \n","from tqdm.notebook import tqdm\n","import tensorflow_addons as tfa\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","from IPython.display import HTML\n","from tensorflow.keras import layers\n","import matplotlib.animation as animation\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split, GroupShuffleSplit "]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:10:10.972664Z","iopub.status.busy":"2023-08-17T12:10:10.971981Z","iopub.status.idle":"2023-08-17T12:10:10.979636Z","shell.execute_reply":"2023-08-17T12:10:10.978039Z","shell.execute_reply.started":"2023-08-17T12:10:10.972639Z"},"trusted":true},"outputs":[],"source":["# If True, processing data from scratch\n","# If False, loads preprocessed data\n","PREPROCESS_DATA = True\n","TRAIN_MODEL = True\n","# True: use 10% of participants as validation set\n","# False: use all data for training -> gives better LB result\n","USE_VAL = False\n","N_ROWS = 543\n","N_DIMS = 3\n","DIM_NAMES = ['x', 'y', 'z']\n","SEED = 42\n","NUM_CLASSES = 250\n","INPUT_SIZE = 64\n","BATCH_ALL_SIGNS_N = 3\n","BATCH_SIZE = 512\n","# LR_MAX = 1e-3\n","LR_MAX = 0.005 # #3\n","# LR_MAX = 0.00155 # #3\n","N_WARMUP_EPOCHS = 50\n","WD_RATIO = 0.05\n","MASK_VAL = 4237\n","N_EPOCHS = 250\n","VERBOSE = 1"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:10:41.038175Z","iopub.status.busy":"2023-08-17T12:10:41.037789Z","iopub.status.idle":"2023-08-17T12:10:41.045486Z","shell.execute_reply":"2023-08-17T12:10:41.044444Z","shell.execute_reply.started":"2023-08-17T12:10:41.038146Z"},"trusted":true},"outputs":[],"source":["USE_TYPES = ['left_hand', 'pose', 'right_hand']\n","LHAND = np.arange(468, 489) # 21\n","RHAND = np.arange(522, 543) # 21\n","# POSE  = np.arange(489, 522)# 33\n","FACE  = np.arange(0,468) #468\n","LIP = np.array([ 0, \n","    61, 185, 40, 39, 37, 267, 269, 270, 409,\n","    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n","    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n","    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n","])\n","LPOSE = np.array([502, 504, 506, 508, 510])\n","RPOSE = np.array([503, 505, 507, 509, 511])\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:10:41.590251Z","iopub.status.busy":"2023-08-17T12:10:41.589889Z","iopub.status.idle":"2023-08-17T12:10:41.598520Z","shell.execute_reply":"2023-08-17T12:10:41.597448Z","shell.execute_reply.started":"2023-08-17T12:10:41.590229Z"},"trusted":true},"outputs":[],"source":["# Concatenate the relevant landmarks for each dominant hand scenario\n","left_hand_dominant = np.concatenate((LIP, LHAND, LPOSE))\n","right_hand_dominant = np.concatenate((LIP, RHAND, RPOSE))\n","# landmarks = np.concatenate((LIP, RHAND, LHAND, POSE))\n","hand_index = np.concatenate((LHAND, RHAND), axis = 0)\n","\n","#Landmark Indices in preprocess data\n","hands_index = np.argwhere(np.isin(left_hand_dominant, hand_index)).squeeze()\n","lip_index = np.argwhere(np.isin(left_hand_dominant, LIP)).squeeze()\n","left_hand_index = np.argwhere(np.isin(left_hand_dominant, LHAND)).squeeze()\n","right_hand_index = np.argwhere(np.isin(left_hand_dominant, RHAND)).squeeze()\n","pose_index = np.argwhere(np.isin(left_hand_dominant, LPOSE)).squeeze()\n","n_cols = left_hand_dominant.size"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:10:43.462750Z","iopub.status.busy":"2023-08-17T12:10:43.462420Z","iopub.status.idle":"2023-08-17T12:10:43.467661Z","shell.execute_reply":"2023-08-17T12:10:43.466584Z","shell.execute_reply.started":"2023-08-17T12:10:43.462725Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["# HAND_IDXS: 21, N_COLS: 66\n"]}],"source":["print(f'# HAND_IDXS: {len(hands_index)}, N_COLS: {n_cols}')"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:10:43.890063Z","iopub.status.busy":"2023-08-17T12:10:43.889317Z","iopub.status.idle":"2023-08-17T12:10:43.895509Z","shell.execute_reply":"2023-08-17T12:10:43.894160Z","shell.execute_reply.started":"2023-08-17T12:10:43.890036Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["LIPS_START: 0, LEFT_HAND_START: 40, RIGHT_HAND_START: 61, POSE_START: 61\n"]}],"source":["LIPS_START = 0\n","LEFT_HAND_START = lip_index.size\n","RIGHT_HAND_START = LEFT_HAND_START + left_hand_index.size\n","POSE_START = RIGHT_HAND_START + right_hand_index.size\n","print(f'LIPS_START: {LIPS_START}, LEFT_HAND_START: {LEFT_HAND_START}, RIGHT_HAND_START: {RIGHT_HAND_START}, POSE_START: {POSE_START}')"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:23.783225Z","iopub.status.busy":"2023-08-17T12:13:23.782842Z","iopub.status.idle":"2023-08-17T12:13:23.950362Z","shell.execute_reply":"2023-08-17T12:13:23.949504Z","shell.execute_reply.started":"2023-08-17T12:13:23.783196Z"},"trusted":true},"outputs":[],"source":["# BASE_DIR = 'sign_data/asl-signs'\n","BASE_DIR = '/kaggle/input/asl-signs'\n","train = pd.read_csv(f'{BASE_DIR}/train.csv')"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:24.855314Z","iopub.status.busy":"2023-08-17T12:13:24.854608Z","iopub.status.idle":"2023-08-17T12:13:24.964992Z","shell.execute_reply":"2023-08-17T12:13:24.963991Z","shell.execute_reply.started":"2023-08-17T12:13:24.855256Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["N_SAMPLES: 40000\n"]}],"source":["# Read Training Data\n","if PREPROCESS_DATA:\n","    train = pd.read_csv(f'{BASE_DIR}/train.csv').sample(int(40e3), random_state=SEED)\n","else:\n","    train = pd.read_csv(f'{BASE_DIR}/train.csv')\n","\n","N_SAMPLES = len(train)\n","print(f'N_SAMPLES: {N_SAMPLES}')"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:25.050347Z","iopub.status.busy":"2023-08-17T12:13:25.049986Z","iopub.status.idle":"2023-08-17T12:13:25.082338Z","shell.execute_reply":"2023-08-17T12:13:25.080567Z","shell.execute_reply.started":"2023-08-17T12:13:25.050321Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>participant_id</th>\n","      <th>sequence_id</th>\n","      <th>sign</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>56533</th>\n","      <td>train_landmark_files/28656/3311214787.parquet</td>\n","      <td>28656</td>\n","      <td>3311214787</td>\n","      <td>sticky</td>\n","    </tr>\n","    <tr>\n","      <th>63119</th>\n","      <td>train_landmark_files/53618/3588192588.parquet</td>\n","      <td>53618</td>\n","      <td>3588192588</td>\n","      <td>before</td>\n","    </tr>\n","    <tr>\n","      <th>8760</th>\n","      <td>train_landmark_files/4718/1363575346.parquet</td>\n","      <td>4718</td>\n","      <td>1363575346</td>\n","      <td>pretty</td>\n","    </tr>\n","    <tr>\n","      <th>93310</th>\n","      <td>train_landmark_files/37779/951199059.parquet</td>\n","      <td>37779</td>\n","      <td>951199059</td>\n","      <td>hen</td>\n","    </tr>\n","    <tr>\n","      <th>44842</th>\n","      <td>train_landmark_files/36257/283190141.parquet</td>\n","      <td>36257</td>\n","      <td>283190141</td>\n","      <td>tomorrow</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>90202</th>\n","      <td>train_landmark_files/53618/821142908.parquet</td>\n","      <td>53618</td>\n","      <td>821142908</td>\n","      <td>green</td>\n","    </tr>\n","    <tr>\n","      <th>88470</th>\n","      <td>train_landmark_files/25571/75047620.parquet</td>\n","      <td>25571</td>\n","      <td>75047620</td>\n","      <td>will</td>\n","    </tr>\n","    <tr>\n","      <th>46001</th>\n","      <td>train_landmark_files/36257/2879593392.parquet</td>\n","      <td>36257</td>\n","      <td>2879593392</td>\n","      <td>wake</td>\n","    </tr>\n","    <tr>\n","      <th>34340</th>\n","      <td>train_landmark_files/37055/2405753437.parquet</td>\n","      <td>37055</td>\n","      <td>2405753437</td>\n","      <td>glasswindow</td>\n","    </tr>\n","    <tr>\n","      <th>13242</th>\n","      <td>train_landmark_files/55372/1545523839.parquet</td>\n","      <td>55372</td>\n","      <td>1545523839</td>\n","      <td>chocolate</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>40000 rows × 4 columns</p>\n","</div>"],"text/plain":["                                                path  participant_id  \\\n","56533  train_landmark_files/28656/3311214787.parquet           28656   \n","63119  train_landmark_files/53618/3588192588.parquet           53618   \n","8760    train_landmark_files/4718/1363575346.parquet            4718   \n","93310   train_landmark_files/37779/951199059.parquet           37779   \n","44842   train_landmark_files/36257/283190141.parquet           36257   \n","...                                              ...             ...   \n","90202   train_landmark_files/53618/821142908.parquet           53618   \n","88470    train_landmark_files/25571/75047620.parquet           25571   \n","46001  train_landmark_files/36257/2879593392.parquet           36257   \n","34340  train_landmark_files/37055/2405753437.parquet           37055   \n","13242  train_landmark_files/55372/1545523839.parquet           55372   \n","\n","       sequence_id         sign  \n","56533   3311214787       sticky  \n","63119   3588192588       before  \n","8760    1363575346       pretty  \n","93310    951199059          hen  \n","44842    283190141     tomorrow  \n","...            ...          ...  \n","90202    821142908        green  \n","88470     75047620         will  \n","46001   2879593392         wake  \n","34340   2405753437  glasswindow  \n","13242   1545523839    chocolate  \n","\n","[40000 rows x 4 columns]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["train"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:26.245392Z","iopub.status.busy":"2023-08-17T12:13:26.245039Z","iopub.status.idle":"2023-08-17T12:13:26.267795Z","shell.execute_reply":"2023-08-17T12:13:26.266407Z","shell.execute_reply.started":"2023-08-17T12:13:26.245366Z"},"trusted":true},"outputs":[],"source":["# Get complete file path to file\n","def get_file_path(path):\n","    return f'/kaggle/input/asl-signs/{path}'\n","\n","train['file_path'] = train['path'].apply(get_file_path)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:27.048543Z","iopub.status.busy":"2023-08-17T12:13:27.048201Z","iopub.status.idle":"2023-08-17T12:13:27.062112Z","shell.execute_reply":"2023-08-17T12:13:27.060859Z","shell.execute_reply.started":"2023-08-17T12:13:27.048517Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>participant_id</th>\n","      <th>sequence_id</th>\n","      <th>sign</th>\n","      <th>file_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>56533</th>\n","      <td>train_landmark_files/28656/3311214787.parquet</td>\n","      <td>28656</td>\n","      <td>3311214787</td>\n","      <td>sticky</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/2...</td>\n","    </tr>\n","    <tr>\n","      <th>63119</th>\n","      <td>train_landmark_files/53618/3588192588.parquet</td>\n","      <td>53618</td>\n","      <td>3588192588</td>\n","      <td>before</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/5...</td>\n","    </tr>\n","    <tr>\n","      <th>8760</th>\n","      <td>train_landmark_files/4718/1363575346.parquet</td>\n","      <td>4718</td>\n","      <td>1363575346</td>\n","      <td>pretty</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/4...</td>\n","    </tr>\n","    <tr>\n","      <th>93310</th>\n","      <td>train_landmark_files/37779/951199059.parquet</td>\n","      <td>37779</td>\n","      <td>951199059</td>\n","      <td>hen</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/3...</td>\n","    </tr>\n","    <tr>\n","      <th>44842</th>\n","      <td>train_landmark_files/36257/283190141.parquet</td>\n","      <td>36257</td>\n","      <td>283190141</td>\n","      <td>tomorrow</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/3...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>90202</th>\n","      <td>train_landmark_files/53618/821142908.parquet</td>\n","      <td>53618</td>\n","      <td>821142908</td>\n","      <td>green</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/5...</td>\n","    </tr>\n","    <tr>\n","      <th>88470</th>\n","      <td>train_landmark_files/25571/75047620.parquet</td>\n","      <td>25571</td>\n","      <td>75047620</td>\n","      <td>will</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/2...</td>\n","    </tr>\n","    <tr>\n","      <th>46001</th>\n","      <td>train_landmark_files/36257/2879593392.parquet</td>\n","      <td>36257</td>\n","      <td>2879593392</td>\n","      <td>wake</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/3...</td>\n","    </tr>\n","    <tr>\n","      <th>34340</th>\n","      <td>train_landmark_files/37055/2405753437.parquet</td>\n","      <td>37055</td>\n","      <td>2405753437</td>\n","      <td>glasswindow</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/3...</td>\n","    </tr>\n","    <tr>\n","      <th>13242</th>\n","      <td>train_landmark_files/55372/1545523839.parquet</td>\n","      <td>55372</td>\n","      <td>1545523839</td>\n","      <td>chocolate</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/5...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>40000 rows × 5 columns</p>\n","</div>"],"text/plain":["                                                path  participant_id  \\\n","56533  train_landmark_files/28656/3311214787.parquet           28656   \n","63119  train_landmark_files/53618/3588192588.parquet           53618   \n","8760    train_landmark_files/4718/1363575346.parquet            4718   \n","93310   train_landmark_files/37779/951199059.parquet           37779   \n","44842   train_landmark_files/36257/283190141.parquet           36257   \n","...                                              ...             ...   \n","90202   train_landmark_files/53618/821142908.parquet           53618   \n","88470    train_landmark_files/25571/75047620.parquet           25571   \n","46001  train_landmark_files/36257/2879593392.parquet           36257   \n","34340  train_landmark_files/37055/2405753437.parquet           37055   \n","13242  train_landmark_files/55372/1545523839.parquet           55372   \n","\n","       sequence_id         sign  \\\n","56533   3311214787       sticky   \n","63119   3588192588       before   \n","8760    1363575346       pretty   \n","93310    951199059          hen   \n","44842    283190141     tomorrow   \n","...            ...          ...   \n","90202    821142908        green   \n","88470     75047620         will   \n","46001   2879593392         wake   \n","34340   2405753437  glasswindow   \n","13242   1545523839    chocolate   \n","\n","                                               file_path  \n","56533  /kaggle/input/asl-signs/train_landmark_files/2...  \n","63119  /kaggle/input/asl-signs/train_landmark_files/5...  \n","8760   /kaggle/input/asl-signs/train_landmark_files/4...  \n","93310  /kaggle/input/asl-signs/train_landmark_files/3...  \n","44842  /kaggle/input/asl-signs/train_landmark_files/3...  \n","...                                                  ...  \n","90202  /kaggle/input/asl-signs/train_landmark_files/5...  \n","88470  /kaggle/input/asl-signs/train_landmark_files/2...  \n","46001  /kaggle/input/asl-signs/train_landmark_files/3...  \n","34340  /kaggle/input/asl-signs/train_landmark_files/3...  \n","13242  /kaggle/input/asl-signs/train_landmark_files/5...  \n","\n","[40000 rows x 5 columns]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["train"]},{"cell_type":"markdown","metadata":{},"source":["### Ordinal Encode Sign"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:29.725398Z","iopub.status.busy":"2023-08-17T12:13:29.725037Z","iopub.status.idle":"2023-08-17T12:13:29.731876Z","shell.execute_reply":"2023-08-17T12:13:29.731159Z","shell.execute_reply.started":"2023-08-17T12:13:29.725348Z"},"trusted":true},"outputs":[],"source":["import json\n","\n","with open('/kaggle/input/asl-signs/sign_to_prediction_index_map.json') as f: \n","    sign_map = json.load(f)\n","\n","signs = list(sign_map)\n"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:31.282789Z","iopub.status.busy":"2023-08-17T12:13:31.282040Z","iopub.status.idle":"2023-08-17T12:13:31.288417Z","shell.execute_reply":"2023-08-17T12:13:31.287346Z","shell.execute_reply.started":"2023-08-17T12:13:31.282758Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['TV', 'after', 'airplane', 'all', 'alligator']"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["signs[:5]"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:32.293243Z","iopub.status.busy":"2023-08-17T12:13:32.292657Z","iopub.status.idle":"2023-08-17T12:13:32.315693Z","shell.execute_reply":"2023-08-17T12:13:32.314460Z","shell.execute_reply.started":"2023-08-17T12:13:32.293190Z"},"trusted":true},"outputs":[],"source":["def mapping_sign_code(sign: str):\n","    return sign_map[sign]\n","train['sign_code'] = train['sign'].apply(mapping_sign_code)"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:33.106364Z","iopub.status.busy":"2023-08-17T12:13:33.106043Z","iopub.status.idle":"2023-08-17T12:13:33.121703Z","shell.execute_reply":"2023-08-17T12:13:33.120703Z","shell.execute_reply.started":"2023-08-17T12:13:33.106342Z"},"trusted":true},"outputs":[],"source":["# Instantiate the encoder\n","le = LabelEncoder()\n","\n","# Fit the encoder and transform the 'sign' column\n","train['sign_code'] = le.fit_transform(train['sign'])\n","\n","# Create dictionaries for mapping\n","SIGN2ORD = dict(zip(le.classes_, le.transform(le.classes_)))\n","ORD2SIGN = dict(zip(le.transform(le.classes_), le.classes_))"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:34.106875Z","iopub.status.busy":"2023-08-17T12:13:34.106538Z","iopub.status.idle":"2023-08-17T12:13:34.122954Z","shell.execute_reply":"2023-08-17T12:13:34.122055Z","shell.execute_reply.started":"2023-08-17T12:13:34.106850Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>participant_id</th>\n","      <th>sequence_id</th>\n","      <th>sign</th>\n","      <th>file_path</th>\n","      <th>sign_code</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>56533</th>\n","      <td>train_landmark_files/28656/3311214787.parquet</td>\n","      <td>28656</td>\n","      <td>3311214787</td>\n","      <td>sticky</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/2...</td>\n","      <td>206</td>\n","    </tr>\n","    <tr>\n","      <th>63119</th>\n","      <td>train_landmark_files/53618/3588192588.parquet</td>\n","      <td>53618</td>\n","      <td>3588192588</td>\n","      <td>before</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/5...</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>8760</th>\n","      <td>train_landmark_files/4718/1363575346.parquet</td>\n","      <td>4718</td>\n","      <td>1363575346</td>\n","      <td>pretty</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/4...</td>\n","      <td>178</td>\n","    </tr>\n","    <tr>\n","      <th>93310</th>\n","      <td>train_landmark_files/37779/951199059.parquet</td>\n","      <td>37779</td>\n","      <td>951199059</td>\n","      <td>hen</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/3...</td>\n","      <td>114</td>\n","    </tr>\n","    <tr>\n","      <th>44842</th>\n","      <td>train_landmark_files/36257/283190141.parquet</td>\n","      <td>36257</td>\n","      <td>283190141</td>\n","      <td>tomorrow</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/3...</td>\n","      <td>221</td>\n","    </tr>\n","    <tr>\n","      <th>20993</th>\n","      <td>train_landmark_files/61333/186594661.parquet</td>\n","      <td>61333</td>\n","      <td>186594661</td>\n","      <td>up</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/6...</td>\n","      <td>230</td>\n","    </tr>\n","    <tr>\n","      <th>89267</th>\n","      <td>train_landmark_files/53618/782770724.parquet</td>\n","      <td>53618</td>\n","      <td>782770724</td>\n","      <td>blow</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/5...</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>48370</th>\n","      <td>train_landmark_files/16069/2977903115.parquet</td>\n","      <td>16069</td>\n","      <td>2977903115</td>\n","      <td>weus</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/1...</td>\n","      <td>236</td>\n","    </tr>\n","    <tr>\n","      <th>41330</th>\n","      <td>train_landmark_files/2044/269101282.parquet</td>\n","      <td>2044</td>\n","      <td>269101282</td>\n","      <td>read</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/2...</td>\n","      <td>184</td>\n","    </tr>\n","    <tr>\n","      <th>53090</th>\n","      <td>train_landmark_files/28656/3171133897.parquet</td>\n","      <td>28656</td>\n","      <td>3171133897</td>\n","      <td>say</td>\n","      <td>/kaggle/input/asl-signs/train_landmark_files/2...</td>\n","      <td>191</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                path  participant_id  \\\n","56533  train_landmark_files/28656/3311214787.parquet           28656   \n","63119  train_landmark_files/53618/3588192588.parquet           53618   \n","8760    train_landmark_files/4718/1363575346.parquet            4718   \n","93310   train_landmark_files/37779/951199059.parquet           37779   \n","44842   train_landmark_files/36257/283190141.parquet           36257   \n","20993   train_landmark_files/61333/186594661.parquet           61333   \n","89267   train_landmark_files/53618/782770724.parquet           53618   \n","48370  train_landmark_files/16069/2977903115.parquet           16069   \n","41330    train_landmark_files/2044/269101282.parquet            2044   \n","53090  train_landmark_files/28656/3171133897.parquet           28656   \n","\n","       sequence_id      sign  \\\n","56533   3311214787    sticky   \n","63119   3588192588    before   \n","8760    1363575346    pretty   \n","93310    951199059       hen   \n","44842    283190141  tomorrow   \n","20993    186594661        up   \n","89267    782770724      blow   \n","48370   2977903115      weus   \n","41330    269101282      read   \n","53090   3171133897       say   \n","\n","                                               file_path  sign_code  \n","56533  /kaggle/input/asl-signs/train_landmark_files/2...        206  \n","63119  /kaggle/input/asl-signs/train_landmark_files/5...         20  \n","8760   /kaggle/input/asl-signs/train_landmark_files/4...        178  \n","93310  /kaggle/input/asl-signs/train_landmark_files/3...        114  \n","44842  /kaggle/input/asl-signs/train_landmark_files/3...        221  \n","20993  /kaggle/input/asl-signs/train_landmark_files/6...        230  \n","89267  /kaggle/input/asl-signs/train_landmark_files/5...         25  \n","48370  /kaggle/input/asl-signs/train_landmark_files/1...        236  \n","41330  /kaggle/input/asl-signs/train_landmark_files/2...        184  \n","53090  /kaggle/input/asl-signs/train_landmark_files/2...        191  "]},"metadata":{},"output_type":"display_data"}],"source":["display(train.head(10))"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:34.931380Z","iopub.status.busy":"2023-08-17T12:13:34.931044Z","iopub.status.idle":"2023-08-17T12:13:34.936604Z","shell.execute_reply":"2023-08-17T12:13:34.935946Z","shell.execute_reply.started":"2023-08-17T12:13:34.931356Z"},"trusted":true},"outputs":[],"source":["ROWS_PER_FRAME = 543  \n","\n","def load_relevant_data_subset(pq_path):\n","    data_columns = ['x', 'y', 'z']\n","    data = pd.read_parquet(pq_path, columns=data_columns)\n","    n_frames = int(len(data) / ROWS_PER_FRAME)\n","    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n","    return data.astype(np.float32)"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:35.642533Z","iopub.status.busy":"2023-08-17T12:13:35.641523Z","iopub.status.idle":"2023-08-17T12:13:35.647837Z","shell.execute_reply":"2023-08-17T12:13:35.646798Z","shell.execute_reply.started":"2023-08-17T12:13:35.642490Z"},"trusted":true},"outputs":[],"source":["def pad_edge(t, repeats, side):\n","    if side == 'LEFT':\n","        return tf.concat((tf.repeat(t[:1], repeats=repeats, axis=0), t), axis=0)\n","    elif side == 'RIGHT':\n","        return tf.concat((t, tf.repeat(t[-1:], repeats=repeats, axis=0)), axis=0)\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:36.687642Z","iopub.status.busy":"2023-08-17T12:13:36.687248Z","iopub.status.idle":"2023-08-17T12:13:36.707646Z","shell.execute_reply":"2023-08-17T12:13:36.706611Z","shell.execute_reply.started":"2023-08-17T12:13:36.687613Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Tensorflow layer to process data within TFLite.\n","To avoid external dependencies, data processing is embedded within the model.\n","\"\"\"\n","class TFLitePreprocessLayer(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(TFLitePreprocessLayer, self).__init__()\n","\n","    @tf.function(input_signature=(tf.TensorSpec(shape=[None, N_ROWS, N_DIMS], dtype=tf.float32),))\n","    def call(self, inputData):\n","\n","        totalFrames = tf.shape(inputData)[0]\n","\n","        # Calculate the dominant hand based on sum of absolute coordinates\n","        leftSum = tf.math.reduce_sum(tf.where(tf.math.is_nan(tf.gather(inputData, LHAND, axis=1)), 0, 1))\n","        rightSum = tf.math.reduce_sum(tf.where(tf.math.is_nan(tf.gather(inputData, RHAND, axis=1)), 0, 1))\n","        isLeftDominant = leftSum >= rightSum\n","\n","        handIndices = LHAND if isLeftDominant else RHAND\n","        framesWithHand = tf.math.reduce_sum(tf.where(tf.math.is_nan(tf.gather(inputData, handIndices, axis=1)), 0, 1), axis=[1, 2])\n","        validFrameIndices = tf.squeeze(tf.where(framesWithHand > 0), axis=1)\n","\n","        processedData = tf.gather(inputData, validFrameIndices, axis=0)\n","        validFrameIndices = tf.cast(validFrameIndices, tf.float32)\n","        validFrameIndices -= tf.reduce_min(validFrameIndices)\n","\n","        frameCount = tf.shape(processedData)[0]\n","\n","        landmarkColumns = left_hand_dominant if isLeftDominant else right_hand_dominant\n","        processedData = tf.gather(processedData, landmarkColumns, axis=1)\n","\n","        # Check if video fits within the input size\n","        if frameCount < INPUT_SIZE:\n","            validFrameIndices = tf.pad(validFrameIndices, [[0, INPUT_SIZE - frameCount]], constant_values=-1)\n","            processedData = tf.pad(processedData, [[0, INPUT_SIZE - frameCount], [0, 0], [0, 0]], constant_values=0)\n","            processedData = tf.where(tf.math.is_nan(processedData), 0.0, processedData)\n","            return processedData, validFrameIndices\n","        else:\n","            # Handling videos larger than the input size\n","            if frameCount < INPUT_SIZE ** 2:\n","                factor = tf.math.floordiv(INPUT_SIZE * INPUT_SIZE, totalFrames)\n","                processedData = tf.repeat(processedData, repeats=factor, axis=0)\n","                validFrameIndices = tf.repeat(validFrameIndices, repeats=factor, axis=0)\n","\n","            adjustedSize = tf.math.floordiv(len(processedData), INPUT_SIZE)\n","            if tf.math.mod(len(processedData), INPUT_SIZE) > 0:\n","                adjustedSize += 1\n","\n","            requiredPadding = (adjustedSize * INPUT_SIZE) - len(processedData) if adjustedSize == 1 else (adjustedSize * INPUT_SIZE) % len(processedData)\n","            padLeft = tf.math.floordiv(requiredPadding, 2) + tf.math.floordiv(INPUT_SIZE, 2)\n","            padRight = padLeft + (1 if tf.math.mod(requiredPadding, 2) > 0 else 0)\n","\n","            processedData = self._addPadding(processedData, padLeft, 'LEFT')\n","            processedData = self._addPadding(processedData, padRight, 'RIGHT')\n","            validFrameIndices = self._addPadding(validFrameIndices, padLeft, 'LEFT')\n","            validFrameIndices = self._addPadding(validFrameIndices, padRight, 'RIGHT')\n","\n","            processedData = tf.reshape(processedData, [INPUT_SIZE, -1, n_cols, N_DIMS])\n","            validFrameIndices = tf.reshape(validFrameIndices, [INPUT_SIZE, -1])\n","            \n","            processedData = tf.experimental.numpy.nanmean(processedData, axis=1)\n","            validFrameIndices = tf.experimental.numpy.nanmean(validFrameIndices, axis=1)\n","\n","            processedData = tf.where(tf.math.is_nan(processedData), 0.0, processedData)\n","            return processedData, validFrameIndices\n","\n","    def _addPadding(self, tensor, paddingSize, direction):\n","        if direction == 'LEFT':\n","            return tf.concat([tf.repeat(tensor[:1], repeats=paddingSize, axis=0), tensor], axis=0)\n","        else:\n","            return tf.concat([tensor, tf.repeat(tensor[-1:], repeats=paddingSize, axis=0)], axis=0)\n","\n","layerInstance = TFLitePreprocessLayer()\n"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:37.573611Z","iopub.status.busy":"2023-08-17T12:13:37.573286Z","iopub.status.idle":"2023-08-17T12:13:37.578786Z","shell.execute_reply":"2023-08-17T12:13:37.577672Z","shell.execute_reply.started":"2023-08-17T12:13:37.573584Z"},"trusted":true},"outputs":[],"source":["def get_data(file_path): \n","    # Load Raw Data\n","    data_2 = load_relevant_data_subset(file_path)\n","    # Process Data Using Tensorflow\n","    data_2 = layerInstance(data_2)\n","    return data_2"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-08-17T12:13:38.604626Z","iopub.status.busy":"2023-08-17T12:13:38.604257Z","iopub.status.idle":"2023-08-17T12:31:58.263798Z","shell.execute_reply":"2023-08-17T12:31:58.262249Z","shell.execute_reply.started":"2023-08-17T12:13:38.604598Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9375df00cb6d43c6b6871ae6427b1571","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Patient ID Intersection Train/Val: set()\n","X_train shape: (30550, 64, 66, 3), X_test shape: (5429, 64, 66, 3), X_val shape: (4021, 64, 66, 3)\n","y_train shape: (30550,), y_test shape: (5429,), y_val shape: (4021,)\n"]}],"source":["# def create_dataset(N_SAMPLES, INPUT_SIZE, N_COLS, N_DIMS, file_paths, sign_ords, participant_ids, SEED=42):\n","def create_dataset():\n","    # Create arrays to save data\n","    X = np.zeros([N_SAMPLES,  INPUT_SIZE, n_cols, N_DIMS], dtype=np.float32)\n","    y = np.zeros([N_SAMPLES ], dtype=np.int32)\n","    NON_EMPTY_FRAME_IDXS = np.full([N_SAMPLES, INPUT_SIZE], -1, dtype=np.float32)\n","\n","    # Fill X/y\n","    for row_idx, (file_path, sign_code) in enumerate(tqdm(train[['file_path', 'sign_code']].itertuples(index=False))):\n","        # Log message every 5000 samples\n","        if row_idx == N_SAMPLES: \n","            break\n","        data, non_empty_frame_idxs = get_data(file_path)\n","        X[row_idx] = data\n","        y[row_idx] = sign_code\n","        NON_EMPTY_FRAME_IDXS[row_idx] = non_empty_frame_idxs\n","        # Sanity check, data should not contain NaN values\n","        if np.isnan(data).sum() > 0:\n","            print(row_idx)\n","            return data\n","\n","    # Save X/y\n","    np.save('X.npy', X)\n","    np.save('y.npy', y)\n","    np.save('NON_EMPTY_FRAME_IDXS.npy', NON_EMPTY_FRAME_IDXS)\n","\n","    # Use GroupShuffleSplit to get train and a temporary set (temp_set)\n","    splitter_1 = GroupShuffleSplit(test_size=0.20, n_splits=1, random_state=SEED)  # Assuming 20% is for validation+test\n","    participant_ids = train['participant_id'].values\n","    train_idxs, temp_idxs = next(splitter_1.split(X, y, groups=participant_ids))\n","\n","    # Further split temp_set into validation and test sets\n","    splitter_2 = GroupShuffleSplit(test_size=0.50, n_splits=1, random_state=SEED)  # Splitting temp_set into half for val and test\n","    X_temp, y_temp, participant_ids_temp = X[temp_idxs], y[temp_idxs], participant_ids[temp_idxs]\n","    val_idxs_temp, test_idxs_temp = next(splitter_2.split(X_temp, y_temp, groups=participant_ids_temp))\n","\n","    # Convert relative val and test indices to original indices\n","    val_idxs = temp_idxs[val_idxs_temp]\n","    test_idxs = temp_idxs[test_idxs_temp]\n","    \n","    # Save Train\n","    X_train = X[train_idxs]\n","    NON_EMPTY_FRAME_IDXS_TRAIN = NON_EMPTY_FRAME_IDXS[train_idxs]\n","    y_train = y[train_idxs]\n","    np.save('X_train.npy', X_train)\n","    np.save('y_train.npy', y_train)\n","    np.save('NON_EMPTY_FRAME_IDXS_TRAIN.npy', NON_EMPTY_FRAME_IDXS_TRAIN)\n","\n","    # Save Validation\n","    X_val = X[val_idxs]\n","    NON_EMPTY_FRAME_IDXS_VAL = NON_EMPTY_FRAME_IDXS[val_idxs]\n","    y_val = y[val_idxs]\n","    np.save('X_val.npy', X_val)\n","    np.save('y_val.npy', y_val)\n","    np.save('NON_EMPTY_FRAME_IDXS_VAL.npy', NON_EMPTY_FRAME_IDXS_VAL)\n","\n","    # Save Test\n","    X_test = X[test_idxs]\n","    NON_EMPTY_FRAME_IDXS_TEST = NON_EMPTY_FRAME_IDXS[test_idxs]\n","    y_test = y[test_idxs]\n","    np.save('X_test.npy', X_test)\n","    np.save('y_test.npy', y_test)\n","    np.save('NON_EMPTY_FRAME_IDXS_TEST.npy', NON_EMPTY_FRAME_IDXS_TEST)\n","    \n","    # Split Statistics\n","    print(f'Patient ID Intersection Train/Val: {set(participant_ids[train_idxs]).intersection(participant_ids[val_idxs])}')\n","    print(f'X_train shape: {X_train.shape}, X_test shape: {X_test.shape}, X_val shape: {X_val.shape}')\n","    print(f'y_train shape: {y_train.shape}, y_test shape: {y_test.shape}, y_val shape: {y_val.shape}')\n","\n","create_dataset()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.8 ('sign_env': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"edba1912e7df98e4246ed3df6861fc9947233ab6c3230f71424d48c234bcfe9a"}}},"nbformat":4,"nbformat_minor":4}
